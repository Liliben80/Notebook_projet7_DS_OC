{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Projet 7 - Notebook des pr√©traitements\n#### Contexte : formation data scientist Openclassrooms\n#### Author : Linda Ben Ali \n#### Date : July 2022","metadata":{"id":"Il8pVMGpX_9y"}},{"cell_type":"markdown","source":"This notebook presents the preprocessing of the dataset of project 7 of the data scientist training.","metadata":{}},{"cell_type":"markdown","source":"# Data","metadata":{"id":"ohBYuALcXz9A"}},{"cell_type":"markdown","source":"\nIn this part, we appropriate the dataset by describing the available tables.","metadata":{}},{"cell_type":"code","source":"# Libraries\nimport zipfile # decompression package\nfrom zipfile import ZipFile # ZipFile classe\nimport pandas as pd","metadata":{"id":"29a666b9","executionInfo":{"status":"ok","timestamp":1659372649436,"user_tz":-120,"elapsed":261,"user":{"displayName":"Linda Ben Ali","userId":"02374437887877519579"}},"execution":{"iopub.status.busy":"2022-09-14T19:47:33.933291Z","iopub.execute_input":"2022-09-14T19:47:33.933731Z","iopub.status.idle":"2022-09-14T19:47:33.965711Z","shell.execute_reply.started":"2022-09-14T19:47:33.933645Z","shell.execute_reply":"2022-09-14T19:47:33.964403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Temp directory\n!pwd","metadata":{"id":"HPUY9ENAnIpB","executionInfo":{"status":"ok","timestamp":1659372653701,"user_tz":-120,"elapsed":3900,"user":{"displayName":"Linda Ben Ali","userId":"02374437887877519579"}},"outputId":"71a8f46f-246d-45eb-e69b-3b1d940ed1ec","execution":{"iopub.status.busy":"2022-09-14T19:47:33.967350Z","iopub.execute_input":"2022-09-14T19:47:33.967882Z","iopub.status.idle":"2022-09-14T19:47:35.119829Z","shell.execute_reply.started":"2022-09-14T19:47:33.967850Z","shell.execute_reply":"2022-09-14T19:47:35.118530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Path of working directory\nimport os\nprint(os.listdir(\"../input/d/benalilinda/p7-ben-ali-linda/\"))","metadata":{"execution":{"iopub.status.busy":"2022-09-14T19:47:35.121128Z","iopub.execute_input":"2022-09-14T19:47:35.121526Z","iopub.status.idle":"2022-09-14T19:47:35.133202Z","shell.execute_reply.started":"2022-09-14T19:47:35.121470Z","shell.execute_reply":"2022-09-14T19:47:35.132310Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Free up space by deleting objects\n# del var ","metadata":{"id":"S8lqiba9CG-M","executionInfo":{"status":"ok","timestamp":1659372653701,"user_tz":-120,"elapsed":7,"user":{"displayName":"Linda Ben Ali","userId":"02374437887877519579"}},"execution":{"iopub.status.busy":"2022-09-14T19:47:35.136018Z","iopub.execute_input":"2022-09-14T19:47:35.136731Z","iopub.status.idle":"2022-09-14T19:47:35.140561Z","shell.execute_reply.started":"2022-09-14T19:47:35.136697Z","shell.execute_reply":"2022-09-14T19:47:35.139621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Move files \n\n# import os, shutil, pathlib, fnmatch\n\n# def copy_dir(src: str, dst: str, pattern: str = '*'):\n#     if not os.path.isdir(dst):\n#         pathlib.Path(dst).mkdir(parents=True, exist_ok=True)\n#     for f in fnmatch.filter(os.listdir(src), pattern):\n#         shutil.copy(os.path.join(src, f), os.path.join(dst, f))\n\n# saved_path = \"../input/p7-ben-ali-linda\"  # The path of your old data, which you have uploaded to new kernal as input\n# output_path = \"/kaggle/working/\"  # The output path of your new kernel.\n\n# copy_dir(saved_path, output_path)","metadata":{"execution":{"iopub.status.busy":"2022-09-14T19:47:35.142058Z","iopub.execute_input":"2022-09-14T19:47:35.142378Z","iopub.status.idle":"2022-09-14T19:47:35.152834Z","shell.execute_reply.started":"2022-09-14T19:47:35.142348Z","shell.execute_reply":"2022-09-14T19:47:35.151966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# \"Dataframe\" creation\n\napplication_train = pd.read_csv('../input/d/benalilinda/p7-ben-ali-linda/ProjetMiseenprod-home-credit-default-risk/application_train.csv')\napplication_test = pd.read_csv('../input/d/benalilinda/p7-ben-ali-linda/ProjetMiseenprod-home-credit-default-risk/application_test.csv')\nprevious_application = pd.read_csv(\"../input/d/benalilinda/p7-ben-ali-linda/ProjetMiseenprod-home-credit-default-risk/previous_application.csv\")\nsample_submission = pd.read_csv('../input/d/benalilinda/p7-ben-ali-linda/ProjetMiseenprod-home-credit-default-risk/sample_submission.csv')\n\ninstallments_payments = pd.read_csv('../input/d/benalilinda/p7-ben-ali-linda/ProjetMiseenprod-home-credit-default-risk/installments_payments.csv')\nbureau = pd.read_csv('../input/d/benalilinda/p7-ben-ali-linda/ProjetMiseenprod-home-credit-default-risk/bureau.csv')\nbureau_balance = pd.read_csv('../input/d/benalilinda/p7-ben-ali-linda/ProjetMiseenprod-home-credit-default-risk/bureau_balance.csv')\ncredit_card_balance = pd.read_csv('../input/d/benalilinda/p7-ben-ali-linda/ProjetMiseenprod-home-credit-default-risk/credit_card_balance.csv')\nPOS_CASH_balance = pd.read_csv('../input/d/benalilinda/p7-ben-ali-linda/ProjetMiseenprod-home-credit-default-risk/POS_CASH_balance.csv')\n","metadata":{"id":"e047d793","executionInfo":{"status":"ok","timestamp":1659372707467,"user_tz":-120,"elapsed":53773,"user":{"displayName":"Linda Ben Ali","userId":"02374437887877519579"}},"execution":{"iopub.status.busy":"2022-09-14T19:47:35.154601Z","iopub.execute_input":"2022-09-14T19:47:35.155278Z","iopub.status.idle":"2022-09-14T19:48:47.891065Z","shell.execute_reply.started":"2022-09-14T19:47:35.155244Z","shell.execute_reply":"2022-09-14T19:48:47.888863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Visualization of the tables !","metadata":{"id":"C77v-XFPVYTP"}},{"cell_type":"markdown","source":"\n\nThis is the main table, broken into two files for Train (with TARGET) and Test (without TARGET).\n\nStatic data for all applications. One row represents one loan in our data sample.","metadata":{"id":"gH5MIxyA24QW"}},{"cell_type":"code","source":"application_train.head()","metadata":{"id":"d0bd7af8","outputId":"f652f353-49fd-4207-a29c-f26ec574c759","executionInfo":{"status":"ok","timestamp":1659372707469,"user_tz":-120,"elapsed":31,"user":{"displayName":"Linda Ben Ali","userId":"02374437887877519579"}},"execution":{"iopub.status.busy":"2022-09-14T19:48:47.894265Z","iopub.execute_input":"2022-09-14T19:48:47.894709Z","iopub.status.idle":"2022-09-14T19:48:47.954693Z","shell.execute_reply.started":"2022-09-14T19:48:47.894661Z","shell.execute_reply":"2022-09-14T19:48:47.953444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"application_test.head()","metadata":{"id":"1882fb3a","outputId":"0cfa241d-7425-49ab-f8d0-df7ca25c7b6c","executionInfo":{"status":"ok","timestamp":1659372707469,"user_tz":-120,"elapsed":28,"user":{"displayName":"Linda Ben Ali","userId":"02374437887877519579"}},"execution":{"iopub.status.busy":"2022-09-14T19:48:47.956354Z","iopub.execute_input":"2022-09-14T19:48:47.956766Z","iopub.status.idle":"2022-09-14T19:48:47.988357Z","shell.execute_reply.started":"2022-09-14T19:48:47.956731Z","shell.execute_reply":"2022-09-14T19:48:47.987335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All previous applications for Home Credit loans of clients who have loans in our sample.\n\nThere is one row for each previous application related to loans in our data sample.","metadata":{"id":"D5nMqMVi4wHT"}},{"cell_type":"code","source":"previous_application.head()","metadata":{"id":"c2cb5c9b","outputId":"b3d5223e-684d-4a7c-984a-5d7b4280c5c2","executionInfo":{"status":"ok","timestamp":1659372707952,"user_tz":-120,"elapsed":509,"user":{"displayName":"Linda Ben Ali","userId":"02374437887877519579"}},"execution":{"iopub.status.busy":"2022-09-14T19:48:47.990053Z","iopub.execute_input":"2022-09-14T19:48:47.990563Z","iopub.status.idle":"2022-09-14T19:48:48.024348Z","shell.execute_reply.started":"2022-09-14T19:48:47.990525Z","shell.execute_reply":"2022-09-14T19:48:48.023062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.head()","metadata":{"id":"cd3fbdda","outputId":"082f3c4a-98d2-4cc3-dba3-0c6a2e936897","executionInfo":{"status":"ok","timestamp":1659372707953,"user_tz":-120,"elapsed":26,"user":{"displayName":"Linda Ben Ali","userId":"02374437887877519579"}},"execution":{"iopub.status.busy":"2022-09-14T19:48:48.029521Z","iopub.execute_input":"2022-09-14T19:48:48.029922Z","iopub.status.idle":"2022-09-14T19:48:48.041927Z","shell.execute_reply.started":"2022-09-14T19:48:48.029889Z","shell.execute_reply":"2022-09-14T19:48:48.041027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Repayment history for the previously disbursed credits in Home Credit related to the loans in our sample.\n\nThere is a) one row for every payment that was made plus b) one row each for missed payment.\n\nOne row is equivalent to one payment of one installment OR one installment corresponding to one payment of one previous Home Credit credit related to loans in our sample.","metadata":{"id":"jJjQMsRe5ACY"}},{"cell_type":"code","source":"installments_payments.head()","metadata":{"id":"ce8396f8","outputId":"cc828f3b-0dd4-44f0-afec-1a558f777990","executionInfo":{"status":"ok","timestamp":1659372707954,"user_tz":-120,"elapsed":26,"user":{"displayName":"Linda Ben Ali","userId":"02374437887877519579"}},"execution":{"iopub.status.busy":"2022-09-14T19:48:48.043886Z","iopub.execute_input":"2022-09-14T19:48:48.044547Z","iopub.status.idle":"2022-09-14T19:48:48.061014Z","shell.execute_reply.started":"2022-09-14T19:48:48.044511Z","shell.execute_reply":"2022-09-14T19:48:48.060153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All client's previous credits provided by other financial institutions that were reported to Credit Bureau (for clients who have a loan in our sample).\n\nFor every loan in our sample, there are as many rows as number of credits the client had in Credit Bureau before the application date.","metadata":{"id":"IC1mgTP92vXc"}},{"cell_type":"code","source":"bureau.head()","metadata":{"id":"f4fecf7c","outputId":"23b65068-81c4-4bcd-9b58-dfb35874b49f","executionInfo":{"status":"ok","timestamp":1659372707955,"user_tz":-120,"elapsed":25,"user":{"displayName":"Linda Ben Ali","userId":"02374437887877519579"}},"execution":{"iopub.status.busy":"2022-09-14T19:48:48.062470Z","iopub.execute_input":"2022-09-14T19:48:48.063028Z","iopub.status.idle":"2022-09-14T19:48:48.086959Z","shell.execute_reply.started":"2022-09-14T19:48:48.062994Z","shell.execute_reply":"2022-09-14T19:48:48.085775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Monthly balances of previous credits in Credit Bureau.\nThis table has one row for each month of history of every previous credit reported to Credit Bureau ‚Äì i.e the table has (#loans in sample * # of relative previous credits * # of months where we have some history observable for the previous credits) rows.\n","metadata":{"id":"cQCv6pEp2dTE"}},{"cell_type":"code","source":"bureau_balance.head()","metadata":{"id":"0203fcc0","outputId":"13a3395d-e886-4b8a-f57c-32044e516a5e","executionInfo":{"status":"ok","timestamp":1659372707957,"user_tz":-120,"elapsed":26,"user":{"displayName":"Linda Ben Ali","userId":"02374437887877519579"}},"execution":{"iopub.status.busy":"2022-09-14T19:48:48.090229Z","iopub.execute_input":"2022-09-14T19:48:48.090780Z","iopub.status.idle":"2022-09-14T19:48:48.103795Z","shell.execute_reply.started":"2022-09-14T19:48:48.090649Z","shell.execute_reply":"2022-09-14T19:48:48.102506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Monthly balance snapshots of previous credit cards that the applicant has with Home Credit.\n\nThis table has one row for each month of history of every previous credit in Home Credit (consumer credit and cash loans) related to loans in our sample ‚Äì i.e. the table has (#loans in sample * # of relative previous credit cards * # of months where we have some history observable for the previous credit card) rows.","metadata":{"id":"imIEiZWE4XoT"}},{"cell_type":"code","source":"credit_card_balance.head()","metadata":{"id":"191407a0","outputId":"c58f43e9-eb80-4097-b726-2e2811f2e001","executionInfo":{"status":"ok","timestamp":1659372707958,"user_tz":-120,"elapsed":25,"user":{"displayName":"Linda Ben Ali","userId":"02374437887877519579"}},"execution":{"iopub.status.busy":"2022-09-14T19:48:48.106307Z","iopub.execute_input":"2022-09-14T19:48:48.107148Z","iopub.status.idle":"2022-09-14T19:48:48.138706Z","shell.execute_reply.started":"2022-09-14T19:48:48.107095Z","shell.execute_reply":"2022-09-14T19:48:48.137495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Monthly balance snapshots of previous POS (point of sales) and cash loans that the applicant had with Home Credit.\n\nThis table has one row for each month of history of every previous credit in Home Credit (consumer credit and cash loans) related to loans in our sample ‚Äì i.e. the table has (#loans in sample * # of relative previous credits * # of months in which we have some history observable for the previous credits) rows.","metadata":{}},{"cell_type":"code","source":"POS_CASH_balance.head()","metadata":{"id":"7425695d","outputId":"6098405c-c8a0-4cfe-82df-443444789c7e","executionInfo":{"status":"ok","timestamp":1659372707959,"user_tz":-120,"elapsed":25,"user":{"displayName":"Linda Ben Ali","userId":"02374437887877519579"}},"execution":{"iopub.status.busy":"2022-09-14T19:48:48.140576Z","iopub.execute_input":"2022-09-14T19:48:48.141313Z","iopub.status.idle":"2022-09-14T19:48:48.156731Z","shell.execute_reply.started":"2022-09-14T19:48:48.141256Z","shell.execute_reply":"2022-09-14T19:48:48.155520Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nStructure of tables and data !","metadata":{"id":"LT0DeaBGVxRw"}},{"cell_type":"code","source":"# Table structure \ntables = [\napplication_train,\napplication_test,\nprevious_application,\nsample_submission,\ninstallments_payments,\nbureau,\nbureau_balance,\ncredit_card_balance,\nPOS_CASH_balance\n]\n\ntables_name = [\n\"application_train\",\n\"application_test\",\n\"previous_application\",\n\"sample_submission\",\n\"installments_payments\",\n\"bureau\",\n\"bureau_balance\",\n\"credit_card_balance\",\n\"POS_CASH_balance\"\n]\n\ni = 0\nfor table in tables: # loop by table\n    variables=list(table.columns) \n    del variables[-1]\n    \n    print(\"The table\", tables_name[i] ,\"counts\", table.shape[0], \"occurrences et\", table.shape[1], \"variables.\\n\") \n    \n    print(\"The table variables are :\\n\", variables, \"\\n \\n \")\n    i+=1\n    ","metadata":{"id":"PyPc44PmaGDc","executionInfo":{"status":"ok","timestamp":1659372707960,"user_tz":-120,"elapsed":25,"user":{"displayName":"Linda Ben Ali","userId":"02374437887877519579"}},"outputId":"5e3dd658-acce-4c66-a2de-d49bf7e50802","execution":{"iopub.status.busy":"2022-09-14T19:48:48.158131Z","iopub.execute_input":"2022-09-14T19:48:48.159195Z","iopub.status.idle":"2022-09-14T19:48:48.174936Z","shell.execute_reply.started":"2022-09-14T19:48:48.159145Z","shell.execute_reply":"2022-09-14T19:48:48.173506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Information table\ni = 0\nfor table in tables: \n    print(tables_name[i],\"\\n\")\n    table.info()\n    i+=1","metadata":{"id":"dc2c88c5","outputId":"60006838-3186-405b-ca32-5fec5e62d38f","executionInfo":{"status":"ok","timestamp":1659372709388,"user_tz":-120,"elapsed":1449,"user":{"displayName":"Linda Ben Ali","userId":"02374437887877519579"}},"execution":{"iopub.status.busy":"2022-09-14T19:48:48.176836Z","iopub.execute_input":"2022-09-14T19:48:48.177613Z","iopub.status.idle":"2022-09-14T19:48:49.460410Z","shell.execute_reply.started":"2022-09-14T19:48:48.177574Z","shell.execute_reply":"2022-09-14T19:48:49.459317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Description table\ni = 0\nfor table in tables:\n    print(tables_name[i], \"\\n\", table.describe())\n    i+=1","metadata":{"id":"e778275d","outputId":"696b8556-99f5-42d4-c217-9f7b50417ad1","executionInfo":{"status":"ok","timestamp":1659372729877,"user_tz":-120,"elapsed":20491,"user":{"displayName":"Linda Ben Ali","userId":"02374437887877519579"}},"execution":{"iopub.status.busy":"2022-09-14T19:48:49.461730Z","iopub.execute_input":"2022-09-14T19:48:49.462715Z","iopub.status.idle":"2022-09-14T19:49:08.369763Z","shell.execute_reply.started":"2022-09-14T19:48:49.462677Z","shell.execute_reply":"2022-09-14T19:49:08.368514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory analysis","metadata":{}},{"cell_type":"markdown","source":"\nThis part corresponds to the exploratory analysis prior to the pre-processing of the data.","metadata":{}},{"cell_type":"code","source":"# Necessary librairies\n\n# Numpy and pandas for data manipulation\nimport numpy as np\nimport pandas as pd \n\n# sklearn preprocessing for dealing with categorical variables\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n\n# File system manangement\nimport os\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Data analysis\nfrom sklearn.decomposition import PCA as sklearnPCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn import preprocessing\nfrom sklearn.manifold import TSNE\n\n# Imbalance sampling\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.utils import resample\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.impute import SimpleImputer\n\n\nseed = 10","metadata":{"execution":{"iopub.status.busy":"2022-09-14T19:49:08.371285Z","iopub.execute_input":"2022-09-14T19:49:08.372321Z","iopub.status.idle":"2022-09-14T19:49:09.687645Z","shell.execute_reply.started":"2022-09-14T19:49:08.372284Z","shell.execute_reply":"2022-09-14T19:49:09.686254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training data\nprint('Training data shape: ', application_train.shape)\napplication_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-14T19:49:09.689236Z","iopub.execute_input":"2022-09-14T19:49:09.689605Z","iopub.status.idle":"2022-09-14T19:49:09.719253Z","shell.execute_reply.started":"2022-09-14T19:49:09.689571Z","shell.execute_reply":"2022-09-14T19:49:09.718446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Testing data features\nprint('Testing data shape: ', application_test.shape)\napplication_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-14T19:49:09.720639Z","iopub.execute_input":"2022-09-14T19:49:09.721040Z","iopub.status.idle":"2022-09-14T19:49:09.752129Z","shell.execute_reply.started":"2022-09-14T19:49:09.721008Z","shell.execute_reply":"2022-09-14T19:49:09.750849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"TARGET variable in train data!\\n\", \napplication_train['TARGET'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2022-09-14T19:49:09.753539Z","iopub.execute_input":"2022-09-14T19:49:09.753980Z","iopub.status.idle":"2022-09-14T19:49:09.770227Z","shell.execute_reply.started":"2022-09-14T19:49:09.753946Z","shell.execute_reply":"2022-09-14T19:49:09.768967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"application_train['TARGET'].astype(int).plot.hist(bins=3)\nplt.title(\"Distribution of variable TARGET\")","metadata":{"execution":{"iopub.status.busy":"2022-09-14T19:49:09.771697Z","iopub.execute_input":"2022-09-14T19:49:09.772031Z","iopub.status.idle":"2022-09-14T19:49:10.069781Z","shell.execute_reply.started":"2022-09-14T19:49:09.771999Z","shell.execute_reply":"2022-09-14T19:49:10.068629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_df = pd.DataFrame({'Target':[\"Negative\", \"Positive\"], 'Value':[282686, 24825]})\nax = plot_df.plot.bar(x='Target', y='Value', rot=0, legend=False)","metadata":{"execution":{"iopub.status.busy":"2022-09-14T19:49:10.071297Z","iopub.execute_input":"2022-09-14T19:49:10.072269Z","iopub.status.idle":"2022-09-14T19:49:10.187974Z","shell.execute_reply.started":"2022-09-14T19:49:10.072232Z","shell.execute_reply":"2022-09-14T19:49:10.186796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to calculate missing values by column # Funct \ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n\n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() / len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","metadata":{"execution":{"iopub.status.busy":"2022-09-14T19:49:10.189684Z","iopub.execute_input":"2022-09-14T19:49:10.190451Z","iopub.status.idle":"2022-09-14T19:49:10.200127Z","shell.execute_reply.started":"2022-09-14T19:49:10.190388Z","shell.execute_reply":"2022-09-14T19:49:10.198945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Missing values statistics\nmissing_values = missing_values_table(application_train)\nmissing_values.head(20)","metadata":{"execution":{"iopub.status.busy":"2022-09-14T19:49:10.201736Z","iopub.execute_input":"2022-09-14T19:49:10.202898Z","iopub.status.idle":"2022-09-14T19:49:10.759721Z","shell.execute_reply.started":"2022-09-14T19:49:10.202852Z","shell.execute_reply":"2022-09-14T19:49:10.758561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of each type of column\napplication_train.dtypes.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-09-14T19:49:10.762406Z","iopub.execute_input":"2022-09-14T19:49:10.763799Z","iopub.status.idle":"2022-09-14T19:49:10.773365Z","shell.execute_reply.started":"2022-09-14T19:49:10.763748Z","shell.execute_reply":"2022-09-14T19:49:10.772280Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of unique classes in each object column\napplication_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-14T19:49:10.781208Z","iopub.execute_input":"2022-09-14T19:49:10.782782Z","iopub.status.idle":"2022-09-14T19:49:11.180031Z","shell.execute_reply.started":"2022-09-14T19:49:10.782731Z","shell.execute_reply":"2022-09-14T19:49:11.178960Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a label encoder object\nle = LabelEncoder()\nle_count = 0\n\n# Iterate through the columns\nfor col in application_train:\n    if application_train[col].dtype == 'object':\n        # If 2 or fewer unique categories\n        if len(list(application_train[col].unique())) <= 2:\n            # Train on the training data\n            le.fit(application_train[col])\n            # Transform both training and testing data\n            application_train[col] = le.transform(application_train[col])\n            application_test[col] = le.transform(application_test[col])\n            \n            # Keep track of how many columns were label encoded\n            le_count += 1\n            \nprint('%d columns were label encoded.' % le_count)","metadata":{"execution":{"iopub.status.busy":"2022-09-14T19:49:11.181160Z","iopub.execute_input":"2022-09-14T19:49:11.181508Z","iopub.status.idle":"2022-09-14T19:49:11.875866Z","shell.execute_reply.started":"2022-09-14T19:49:11.181478Z","shell.execute_reply":"2022-09-14T19:49:11.874686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# one-hot encoding of categorical variables\napplication_train = pd.get_dummies(application_train)\napplication_test = pd.get_dummies(application_test)\n\nprint('Training Features shape: ', application_train.shape)\nprint('Testing Features shape: ', application_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-09-14T19:49:11.877676Z","iopub.execute_input":"2022-09-14T19:49:11.878144Z","iopub.status.idle":"2022-09-14T19:49:12.860217Z","shell.execute_reply.started":"2022-09-14T19:49:11.878099Z","shell.execute_reply":"2022-09-14T19:49:12.859013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_labels = application_train['TARGET']\n\n# Align the training and testing data, keep only columns present in both dataframes\napplication_train, application_test = application_train.align(application_test, join = 'inner', axis = 1)\n\n# Add the target back in\napplication_train['TARGET'] = train_labels\n\nprint('Training Features shape: ', application_train.shape)\nprint('Testing Features shape: ', application_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-09-14T19:49:12.861771Z","iopub.execute_input":"2022-09-14T19:49:12.862358Z","iopub.status.idle":"2022-09-14T19:49:13.239503Z","shell.execute_reply.started":"2022-09-14T19:49:12.862325Z","shell.execute_reply":"2022-09-14T19:49:13.238236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Focus on the variable DAYS_EMPLOYED\")\napplication_train['DAYS_EMPLOYED'].describe()","metadata":{"execution":{"iopub.status.busy":"2022-09-14T19:49:13.241694Z","iopub.execute_input":"2022-09-14T19:49:13.242177Z","iopub.status.idle":"2022-09-14T19:49:13.265346Z","shell.execute_reply.started":"2022-09-14T19:49:13.242133Z","shell.execute_reply":"2022-09-14T19:49:13.264248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"application_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\nplt.xlabel('Days Employment');","metadata":{"execution":{"iopub.status.busy":"2022-09-14T19:49:13.267297Z","iopub.execute_input":"2022-09-14T19:49:13.267747Z","iopub.status.idle":"2022-09-14T19:49:13.529214Z","shell.execute_reply.started":"2022-09-14T19:49:13.267705Z","shell.execute_reply":"2022-09-14T19:49:13.527823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"anom = application_train[application_train['DAYS_EMPLOYED'] == 365243]\nnon_anom = application_train[application_train['DAYS_EMPLOYED'] != 365243]\nprint('The non-anomalies default on %0.2f%% of loans' % (100 * non_anom['TARGET'].mean()))\nprint('The anomalies default on %0.2f%% of loans' % (100 * anom['TARGET'].mean()))\nprint('There are %d anomalous days of employment' % len(anom))","metadata":{"execution":{"iopub.status.busy":"2022-09-14T19:49:13.531837Z","iopub.execute_input":"2022-09-14T19:49:13.532208Z","iopub.status.idle":"2022-09-14T19:49:14.053791Z","shell.execute_reply.started":"2022-09-14T19:49:13.532168Z","shell.execute_reply":"2022-09-14T19:49:14.052531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create an anomalous flag column\napplication_train['DAYS_EMPLOYED_ANOM'] = application_train[\"DAYS_EMPLOYED\"] == 365243\n\n# Replace the anomalous values with nan\napplication_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n\napplication_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\nplt.xlabel('Days Employment');","metadata":{"execution":{"iopub.status.busy":"2022-09-14T19:49:14.055878Z","iopub.execute_input":"2022-09-14T19:49:14.056273Z","iopub.status.idle":"2022-09-14T19:49:14.312790Z","shell.execute_reply.started":"2022-09-14T19:49:14.056205Z","shell.execute_reply":"2022-09-14T19:49:14.311471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"application_test['DAYS_EMPLOYED_ANOM'] = application_test[\"DAYS_EMPLOYED\"] == 365243\napplication_test[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)\n\nprint('There are %d anomalies in the test data out of %d entries' % (application_test[\"DAYS_EMPLOYED_ANOM\"].sum(), len(application_test)))","metadata":{"execution":{"iopub.status.busy":"2022-09-14T19:49:14.315778Z","iopub.execute_input":"2022-09-14T19:49:14.316627Z","iopub.status.idle":"2022-09-14T19:49:14.333591Z","shell.execute_reply.started":"2022-09-14T19:49:14.316576Z","shell.execute_reply":"2022-09-14T19:49:14.332282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop the target from the training data\ntrainY = application_train['TARGET']\n\nif 'TARGET' in application_train:\n    trainX = application_train.drop(columns = ['TARGET'])\nelse:\n    trainX = apllication_train.copy()\n    \n# Feature names\nfeatures = list(trainX.columns)\n\n# Copy of the testing data\ntest = application_test.copy()\n\n# Median imputation of missing values\nimputer = SimpleImputer(strategy = 'median')\n\n# Scale each feature to 0-1\nscaler = MinMaxScaler(feature_range = (0, 1))\n\n# Fit on the training data\nimputer.fit(trainX)\n\n# Transform both training and testing data\ntrainX = imputer.transform(trainX)\ntest = imputer.transform(application_test)\n\nnew_app_train = pd.DataFrame(data=trainX[:,:], columns=features[:])\nnew_app_train['TARGET'] = trainY\n\nnew_app_test = pd.DataFrame(data=test[:,:], columns=features[:])\n\n\nprint(trainX.shape)\nprint(trainY.shape)\nprint(new_app_train.shape)\nprint(type(new_app_train))\nmissing_values = missing_values_table(new_app_train)","metadata":{"execution":{"iopub.status.busy":"2022-09-14T19:49:14.335367Z","iopub.execute_input":"2022-09-14T19:49:14.335763Z","iopub.status.idle":"2022-09-14T19:49:47.234252Z","shell.execute_reply.started":"2022-09-14T19:49:14.335728Z","shell.execute_reply":"2022-09-14T19:49:47.232908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creation of training and test data\nX_train, X_test, y_train, y_test = train_test_split(trainX, trainY, test_size=0.33, random_state=seed)","metadata":{"execution":{"iopub.status.busy":"2022-09-14T19:49:47.235830Z","iopub.execute_input":"2022-09-14T19:49:47.236288Z","iopub.status.idle":"2022-09-14T19:49:48.280450Z","shell.execute_reply.started":"2022-09-14T19:49:47.236243Z","shell.execute_reply":"2022-09-14T19:49:48.279229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_values = missing_values_table(pd.DataFrame(trainX))\nmissing_values.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-14T19:49:48.282191Z","iopub.execute_input":"2022-09-14T19:49:48.282955Z","iopub.status.idle":"2022-09-14T19:49:48.583925Z","shell.execute_reply.started":"2022-09-14T19:49:48.282918Z","shell.execute_reply":"2022-09-14T19:49:48.582779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Standardization\")\nscalar = preprocessing.StandardScaler().fit(X_train)\nX_train_norm = pd.DataFrame(scalar.transform(X_train))\ny_train_norm = y_train.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-09-14T19:49:48.585416Z","iopub.execute_input":"2022-09-14T19:49:48.585790Z","iopub.status.idle":"2022-09-14T19:49:49.335192Z","shell.execute_reply.started":"2022-09-14T19:49:48.585759Z","shell.execute_reply":"2022-09-14T19:49:49.333719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = y_train_norm\nX_train = X_train_norm","metadata":{"execution":{"iopub.status.busy":"2022-09-14T19:49:49.336451Z","iopub.execute_input":"2022-09-14T19:49:49.337583Z","iopub.status.idle":"2022-09-14T19:49:49.343152Z","shell.execute_reply.started":"2022-09-14T19:49:49.337543Z","shell.execute_reply":"2022-09-14T19:49:49.342204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"PCA Analysis\")\npca = sklearnPCA(n_components=2) #2-dimensional PCA\ntransformed = pd.DataFrame(pca.fit_transform(X_train_norm))\nprint(transformed.head())","metadata":{"execution":{"iopub.status.busy":"2022-09-14T19:49:49.344905Z","iopub.execute_input":"2022-09-14T19:49:49.347355Z","iopub.status.idle":"2022-09-14T19:49:53.840487Z","shell.execute_reply.started":"2022-09-14T19:49:49.347314Z","shell.execute_reply":"2022-09-14T19:49:53.839175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.scatter(transformed[y_train_norm==0][0], transformed[y_train_norm==0][1], label='No Risk', c='blue')\nplt.scatter(transformed[y_train_norm==1][0], transformed[y_train_norm==1][1], label='Risk', c='red')\n\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-14T19:49:53.842030Z","iopub.execute_input":"2022-09-14T19:49:53.842786Z","iopub.status.idle":"2022-09-14T19:49:55.456585Z","shell.execute_reply.started":"2022-09-14T19:49:53.842750Z","shell.execute_reply":"2022-09-14T19:49:55.455209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"FastICA is an efficient and popular algorithm for independent component analysis.\")\nfrom sklearn.decomposition import FastICA\npca = FastICA(n_components=2) # 2-dimensional PCA\ntransformed2 = pd.DataFrame(pca.fit_transform(X_train_norm))","metadata":{"execution":{"iopub.status.busy":"2022-09-14T19:49:55.458140Z","iopub.execute_input":"2022-09-14T19:49:55.458972Z","iopub.status.idle":"2022-09-14T19:50:12.053250Z","shell.execute_reply.started":"2022-09-14T19:49:55.458926Z","shell.execute_reply":"2022-09-14T19:50:12.051624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.scatter(transformed2[y_train_norm==0][0], transformed2[y_train_norm==0][1], label='No Risk', c='blue')\nplt.scatter(transformed2[y_train_norm==1][0], transformed2[y_train_norm==1][1], label='Risk', c='red')\n\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-14T19:50:12.055779Z","iopub.execute_input":"2022-09-14T19:50:12.056910Z","iopub.status.idle":"2022-09-14T19:50:13.710640Z","shell.execute_reply.started":"2022-09-14T19:50:12.056846Z","shell.execute_reply":"2022-09-14T19:50:13.709699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Conclusion :\nObviously there is no way to separate the red and blue dots with a line.","metadata":{}},{"cell_type":"markdown","source":"# Pre-treatment","metadata":{"id":"AvOepl6Y2A2D"}},{"cell_type":"markdown","source":"\nIn this notebook, we prepare the dataset and perform feature engineering. For this, we downloaded a kaggle kernel \"import lightgbm_with_simple_features\" that we run on our dataset!","metadata":{}},{"cell_type":"code","source":"# Kernel kaggle \n!cp \"../input/d/benalilinda/p7-ben-ali-linda/lightgbm_with_simple_features.py\" \"/kaggle/working/\"\nimport lightgbm_with_simple_features","metadata":{"id":"BmhzP1S0uGmq","executionInfo":{"status":"ok","timestamp":1659372730282,"user_tz":-120,"elapsed":4,"user":{"displayName":"Linda Ben Ali","userId":"02374437887877519579"}},"execution":{"iopub.status.busy":"2022-09-14T19:50:13.712319Z","iopub.execute_input":"2022-09-14T19:50:13.712733Z","iopub.status.idle":"2022-09-14T19:50:16.082504Z","shell.execute_reply.started":"2022-09-14T19:50:13.712698Z","shell.execute_reply":"2022-09-14T19:50:16.081055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# HOME CREDIT DEFAULT RISK COMPETITION\n# Most features are created by applying min, max, mean, sum and var functions to grouped tables. \n# Little feature selection is done and overfitting might be a problem since many features are related.\n# The following key ideas were used:\n# - Divide or subtract important features to get rates (like annuity and income)\n# - In Bureau Data: create specific features for Active credits and Closed credits\n# - In Previous Applications: create specific features for Approved and Refused applications\n# - Modularity: one function for each table (except bureau_balance and application_test)\n# - One-hot encoding for categorical features\n# All tables are joined with the application DF using the SK_ID_CURR key (except bureau_balance).\n# You can use LightGBM with KFold or Stratified KFold.\n\n# Update 16/06/2018:\n# - Added Payment Rate feature\n# - Removed index from features\n# - Use standard KFold CV (not stratified)\n\nimport numpy as np\nimport pandas as pd\nimport gc\nimport time\nimport re\nfrom contextlib import contextmanager # Python encounters the yield keyword\nfrom lightgbm import LGBMClassifier # modelisation\nfrom sklearn.metrics import roc_auc_score, roc_curve # modelisation\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport matplotlib.pyplot as plt # graph\nimport seaborn as sns # graph\nimport warnings # warnings management\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# Timer\n@contextmanager\ndef timer(title):\n    t0 = time.time()\n    yield\n    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n\n# One-hot encoding for categorical columns with get_dummies\ndef one_hot_encoder(df, nan_as_category = True):\n    original_columns = list(df.columns) # variables list\n    categorical_columns = [col for col in df.columns if df[col].dtype == 'object'] # object variable\n    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)  # convert categorical variable into dummy/indicator variables\n    new_columns = [c for c in df.columns if c not in original_columns]\n    return df, new_columns\n\n# Preprocess application_train.csv and application_test.csv\ndef application_train_test(num_rows = None, nan_as_category = False):\n    # Read data and merge\n    df = pd.read_csv('../input/d/benalilinda/p7-ben-ali-linda/ProjetMiseenprod-home-credit-default-risk/application_train.csv', nrows= num_rows)\n#     df = df.sample(frac=0.6, replace=True, random_state=1) # sample for model traitement ==> submission_kernel02\n    test_df = pd.read_csv('../input/d/benalilinda/p7-ben-ali-linda/ProjetMiseenprod-home-credit-default-risk/application_test.csv', nrows= num_rows)\n    print(\"Train samples: {}, test samples: {}\".format(len(df), len(test_df)))\n    df = df.append(test_df).reset_index()\n    # Optional: Remove 4 applications with XNA CODE_GENDER (train set)\n    df = df[df['CODE_GENDER'] != 'XNA']\n    \n    # Categorical features with Binary encode (0 or 1; two categories)\n    for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n        df[bin_feature], uniques = pd.factorize(df[bin_feature])\n    # Categorical features with One-Hot encode\n    df, cat_cols = one_hot_encoder(df, nan_as_category) # encoding\n    \n    # NaN values for DAYS_EMPLOYED: 365.243 -> nan\n    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace= True)\n    # Some simple new features (percentages)\n    df['DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n    df['INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT']\n    df['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n    df['ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n    df['PAYMENT_RATE'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']\n    del test_df\n    gc.collect()\n    return df\n\n# Preprocess bureau.csv and bureau_balance.csv\ndef bureau_and_balance(num_rows = None, nan_as_category = True):\n    bureau = pd.read_csv('../input/d/benalilinda/p7-ben-ali-linda/ProjetMiseenprod-home-credit-default-risk/bureau.csv', nrows = num_rows)\n    bb = pd.read_csv('../input/d/benalilinda/p7-ben-ali-linda/ProjetMiseenprod-home-credit-default-risk/bureau_balance.csv', nrows = num_rows)\n    bb, bb_cat = one_hot_encoder(bb, nan_as_category) # encoding\n    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category) # encoding\n    \n    # Bureau balance: Perform aggregations and merge with bureau.csv\n    bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size']}\n    for col in bb_cat:\n        bb_aggregations[col] = ['mean']\n    bb_agg = bb.groupby('SK_ID_BUREAU').agg(bb_aggregations) # aggregations\n    bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n    bureau = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU') # merge\n    bureau.drop(['SK_ID_BUREAU'], axis=1, inplace= True)\n    del bb, bb_agg\n    gc.collect()\n    \n    # Bureau and bureau_balance numeric features\n    num_aggregations = {\n        'DAYS_CREDIT': ['min', 'max', 'mean', 'var'],\n        'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\n        'DAYS_CREDIT_UPDATE': ['mean'],\n        'CREDIT_DAY_OVERDUE': ['max', 'mean'],\n        'AMT_CREDIT_MAX_OVERDUE': ['mean'],\n        'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n        'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n        'AMT_ANNUITY': ['max', 'mean'],\n        'CNT_CREDIT_PROLONG': ['sum'],\n        'MONTHS_BALANCE_MIN': ['min'],\n        'MONTHS_BALANCE_MAX': ['max'],\n        'MONTHS_BALANCE_SIZE': ['mean', 'sum']\n    }\n    # Bureau and bureau_balance categorical features\n    cat_aggregations = {}\n    for cat in bureau_cat: cat_aggregations[cat] = ['mean']\n    for cat in bb_cat: cat_aggregations[cat + \"_MEAN\"] = ['mean']\n    \n    bureau_agg = bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations}) # aggregations\n    bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n    # Bureau: Active credits - using only numerical aggregations\n    active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n    active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n    active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n    bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR') # merge\n    del active, active_agg\n    gc.collect()\n    # Bureau: Closed credits - using only numerical aggregations\n    closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n    closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations) # aggregations\n    closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n    bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR') # merge\n    del closed, closed_agg, bureau\n    gc.collect()\n    return bureau_agg\n\n# Preprocess previous_applications.csv\ndef previous_applications(num_rows = None, nan_as_category = True):\n    prev = pd.read_csv('../input/d/benalilinda/p7-ben-ali-linda/ProjetMiseenprod-home-credit-default-risk/previous_application.csv', nrows = num_rows)\n    prev, cat_cols = one_hot_encoder(prev, nan_as_category= True) # encoding\n    # Days 365.243 values -> nan\n    prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n    prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n    prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n    prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n    prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n    # Add feature: value ask / value received percentage\n    prev['APP_CREDIT_PERC'] = prev['AMT_APPLICATION'] / prev['AMT_CREDIT']\n    # Previous applications numeric features\n    num_aggregations = {\n        'AMT_ANNUITY': ['min', 'max', 'mean'],\n        'AMT_APPLICATION': ['min', 'max', 'mean'],\n        'AMT_CREDIT': ['min', 'max', 'mean'],\n        'APP_CREDIT_PERC': ['min', 'max', 'mean', 'var'],\n        'AMT_DOWN_PAYMENT': ['min', 'max', 'mean'],\n        'AMT_GOODS_PRICE': ['min', 'max', 'mean'],\n        'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n        'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n        'DAYS_DECISION': ['min', 'max', 'mean'],\n        'CNT_PAYMENT': ['mean', 'sum'],\n    }\n    # Previous applications categorical features\n    cat_aggregations = {}\n    for cat in cat_cols:\n        cat_aggregations[cat] = ['mean']\n    \n    prev_agg = prev.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations}) # aggregations\n    prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n    # Previous Applications: Approved Applications - only numerical features\n    approved = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1]\n    approved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations) # aggregations\n    approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n    prev_agg = prev_agg.join(approved_agg, how='left', on='SK_ID_CURR') # merge\n    # Previous Applications: Refused Applications - only numerical features\n    refused = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1]\n    refused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations) # aggregations\n    refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n    prev_agg = prev_agg.join(refused_agg, how='left', on='SK_ID_CURR') # merge\n    del refused, refused_agg, approved, approved_agg, prev\n    gc.collect()\n    return prev_agg\n\n# Preprocess POS_CASH_balance.csv\ndef pos_cash(num_rows = None, nan_as_category = True):\n    pos = pd.read_csv('../input/d/benalilinda/p7-ben-ali-linda/ProjetMiseenprod-home-credit-default-risk/POS_CASH_balance.csv', nrows = num_rows)\n    pos, cat_cols = one_hot_encoder(pos, nan_as_category= True) # encoding\n    # Features\n    aggregations = {\n        'MONTHS_BALANCE': ['max', 'mean', 'size'],\n        'SK_DPD': ['max', 'mean'],\n        'SK_DPD_DEF': ['max', 'mean']\n    }\n    for cat in cat_cols:\n        aggregations[cat] = ['mean']\n    \n    pos_agg = pos.groupby('SK_ID_CURR').agg(aggregations) # aggregations\n    pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n    # Count pos cash accounts\n    pos_agg['POS_COUNT'] = pos.groupby('SK_ID_CURR').size()\n    del pos\n    gc.collect()\n    return pos_agg\n    \n# Preprocess installments_payments.csv\ndef installments_payments(num_rows = None, nan_as_category = True):\n    ins = pd.read_csv('../input/d/benalilinda/p7-ben-ali-linda/ProjetMiseenprod-home-credit-default-risk/installments_payments.csv', nrows = num_rows)\n    ins, cat_cols = one_hot_encoder(ins, nan_as_category= True) # encoding\n    # Percentage and difference paid in each installment (amount paid and installment value)\n    ins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']\n    ins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n    # Days past due and days before due (no negative values)\n    ins['DPD'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\n    ins['DBD'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\n    ins['DPD'] = ins['DPD'].apply(lambda x: x if x > 0 else 0)\n    ins['DBD'] = ins['DBD'].apply(lambda x: x if x > 0 else 0)\n    # Features: Perform aggregations\n    aggregations = {\n        'NUM_INSTALMENT_VERSION': ['nunique'],\n        'DPD': ['max', 'mean', 'sum'],\n        'DBD': ['max', 'mean', 'sum'],\n        'PAYMENT_PERC': ['max', 'mean', 'sum', 'var'],\n        'PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\n        'AMT_INSTALMENT': ['max', 'mean', 'sum'],\n        'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum']\n    }\n    for cat in cat_cols:\n        aggregations[cat] = ['mean']\n    ins_agg = ins.groupby('SK_ID_CURR').agg(aggregations) # aggregations\n    ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n    # Count installments accounts\n    ins_agg['INSTAL_COUNT'] = ins.groupby('SK_ID_CURR').size()\n    del ins\n    gc.collect()\n    return ins_agg\n\n# Preprocess credit_card_balance.csv\ndef credit_card_balance(num_rows = None, nan_as_category = True):\n    cc = pd.read_csv('../input/d/benalilinda/p7-ben-ali-linda/ProjetMiseenprod-home-credit-default-risk/credit_card_balance.csv', nrows = num_rows)\n    cc, cat_cols = one_hot_encoder(cc, nan_as_category= True) # encoding\n    # General aggregations\n    cc.drop(['SK_ID_PREV'], axis= 1, inplace = True)\n    cc_agg = cc.groupby('SK_ID_CURR').agg(['min', 'max', 'mean', 'sum', 'var']) # aggregations\n    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n    # Count credit card lines\n    cc_agg['CC_COUNT'] = cc.groupby('SK_ID_CURR').size()\n    del cc\n    gc.collect()\n    return cc_agg\n\n# # LightGBM GBDT with KFold or Stratified KFold\n# # Parameters from Tilii kernel: https://www.kaggle.com/tilii7/olivier-lightgbm-parameters-by-bayesian-opt/code\n# def kfold_lightgbm(df, num_folds, stratified = False, debug= False):\n#     # Divide in training/validation and test data\n#     train_df = df[df['TARGET'].notnull()]\n#     test_df = df[df['TARGET'].isnull()]\n#     print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n#     del df\n#     gc.collect()\n#     # Cross validation model\n#     if stratified:\n#         folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=1001)\n#     else:\n#         folds = KFold(n_splits= num_folds, shuffle=True, random_state=1001)\n#     # Create arrays and dataframes to store results\n#     oof_preds = np.zeros(train_df.shape[0])\n#     sub_preds = np.zeros(test_df.shape[0])\n#     feature_importance_df = pd.DataFrame()\n#     feats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n    \n#     for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\n#         train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]\n#         valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]\n\n#         # LightGBM parameters found by Bayesian optimization\n#         clf = LGBMClassifier(\n#             nthread=4,\n#             n_estimators=10000,\n#             learning_rate=0.02,\n#             num_leaves=34,\n#             colsample_bytree=0.9497036,\n#             subsample=0.8715623,\n#             max_depth=8,\n#             reg_alpha=0.041545473,\n#             reg_lambda=0.0735294,\n#             min_split_gain=0.0222415,\n#             min_child_weight=39.3259775,\n#             silent=-1,\n#             verbose=-1, )\n\n#         clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], eval_metric= 'auc', verbose= 200, early_stopping_rounds= 200)\n\n#         oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n#         sub_preds += clf.predict_proba(test_df[feats], num_iteration=clf.best_iteration_)[:, 1] / folds.n_splits\n\n#         fold_importance_df = pd.DataFrame()\n#         fold_importance_df[\"feature\"] = feats\n#         fold_importance_df[\"importance\"] = clf.feature_importances_\n#         fold_importance_df[\"fold\"] = n_fold + 1\n#         feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n#         print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))\n#         del clf, train_x, train_y, valid_x, valid_y\n#         gc.collect()\n\n#     print('Full AUC score %.6f' % roc_auc_score(train_df['TARGET'], oof_preds))\n#     # Write submission file and plot feature importance\n#     if not debug:\n#         test_df['TARGET'] = sub_preds\n#         test_df[['SK_ID_CURR', 'TARGET']].to_csv(submission_file_name, index= False)\n#     display_importances(feature_importance_df)\n#     return feature_importance_df\n\n# # Display/plot feature importance\n# def display_importances(feature_importance_df_):\n#     cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:40].index\n#     best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n#     plt.figure(figsize=(8, 10))\n#     sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n#     plt.title('LightGBM Features (avg over folds)')\n#     plt.tight_layout()\n#     plt.savefig('lgbm_importances01.png')\n\n# Function of launch processing functions\ndef main(debug = False):\n    num_rows = 10000 if debug else None\n    df = application_train_test(num_rows)\n    with timer(\"Process bureau and bureau_balance\"):\n        bureau = bureau_and_balance(num_rows)\n        print(\"Bureau df shape:\", bureau.shape)\n        df = df.join(bureau, how='left', on='SK_ID_CURR') # merge\n        del bureau\n        gc.collect()\n    with timer(\"Process previous_applications\"):\n        prev = previous_applications(num_rows)\n        print(\"Previous applications df shape:\", prev.shape)\n        df = df.join(prev, how='left', on='SK_ID_CURR') # merge\n        del prev\n        gc.collect()\n    with timer(\"Process POS-CASH balance\"):\n        pos = pos_cash(num_rows)\n        print(\"Pos-cash balance df shape:\", pos.shape)\n        df = df.join(pos, how='left', on='SK_ID_CURR') # merge\n        del pos\n        gc.collect()\n    with timer(\"Process installments payments\"):\n        ins = installments_payments(num_rows)\n        print(\"Installments payments df shape:\", ins.shape)\n        df = df.join(ins, how='left', on='SK_ID_CURR') # merge\n        del ins\n        gc.collect()\n    with timer(\"Process credit card balance\"):\n        cc = credit_card_balance(num_rows)\n        print(\"Credit card balance df shape:\", cc.shape)\n        df = df.join(cc, how='left', on='SK_ID_CURR') # merge\n        del cc\n        gc.collect()\n    if not debug:\n        df.to_csv(submission_file_name, index= False)\n#     with timer(\"Run LightGBM with kfold\"):\n#         df = df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n#         feat_importance = kfold_lightgbm(df, num_folds= 10, stratified= False, debug= debug)\n      \n# Launch\nif __name__ == \"__main__\":\n#     submission_file_name = \"submission_kernel02.csv\"\n#     with timer(\"Full model run\"):\n    submission_file_name = \"df.csv\"\n    with timer(\"Feature engineering\"):\n        main()","metadata":{"id":"VrJpcDdS91aI","outputId":"aa52eb11-398d-4da0-f736-aa03daca2cd5","execution":{"iopub.status.busy":"2022-09-14T19:50:16.085351Z","iopub.execute_input":"2022-09-14T19:50:16.085905Z","iopub.status.idle":"2022-09-14T19:55:26.875098Z","shell.execute_reply.started":"2022-09-14T19:50:16.085853Z","shell.execute_reply":"2022-09-14T19:55:26.873824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Temp file creation\n# !touch test.txt","metadata":{"execution":{"iopub.status.busy":"2022-09-14T19:55:26.877142Z","iopub.execute_input":"2022-09-14T19:55:26.877534Z","iopub.status.idle":"2022-09-14T19:55:26.882352Z","shell.execute_reply.started":"2022-09-14T19:55:26.877500Z","shell.execute_reply":"2022-09-14T19:55:26.881019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Directory status change\n# !ls -l ../input/d/benalilinda\n\n# total 4\n# drwxr-xr-x 3 root root 4096 Aug  3 17:48 d # utilisateur - groupe - autre","metadata":{"execution":{"iopub.status.busy":"2022-09-14T19:55:26.884194Z","iopub.execute_input":"2022-09-14T19:55:26.884646Z","iopub.status.idle":"2022-09-14T19:55:26.896534Z","shell.execute_reply.started":"2022-09-14T19:55:26.884609Z","shell.execute_reply":"2022-09-14T19:55:26.895410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Directory status  \n# !chmod -R 775 ../input/d/benalilinda","metadata":{"execution":{"iopub.status.busy":"2022-09-14T19:55:26.898674Z","iopub.execute_input":"2022-09-14T19:55:26.899372Z","iopub.status.idle":"2022-09-14T19:55:26.910010Z","shell.execute_reply.started":"2022-09-14T19:55:26.899322Z","shell.execute_reply":"2022-09-14T19:55:26.908930Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save files results\nimport pandas as pd\ndf = pd.read_csv(\"/kaggle/working/df.csv\")\n# -------------------\n# submission_kernel02 = pd.read_csv(\"/kaggle/working/submission_kernel02.csv\")\n# submission_kernel02.to_csv(\"../input/d/benalilinda/submission_kernel02.csv\")\n\n# !cp \"/kaggle/working/submission_kernel02.csv\" \"../input/d/benalilinda\" # impossible ==> manual treatment !","metadata":{"execution":{"iopub.status.busy":"2022-09-14T19:55:26.911747Z","iopub.execute_input":"2022-09-14T19:55:26.912591Z","iopub.status.idle":"2022-09-14T19:56:03.485659Z","shell.execute_reply.started":"2022-09-14T19:55:26.912544Z","shell.execute_reply":"2022-09-14T19:56:03.484421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-14T19:56:03.488929Z","iopub.execute_input":"2022-09-14T19:56:03.489286Z","iopub.status.idle":"2022-09-14T19:56:03.529692Z","shell.execute_reply.started":"2022-09-14T19:56:03.489248Z","shell.execute_reply":"2022-09-14T19:56:03.528689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At the output of feature engineering, some fields are at Nan or Infinite, which will have to be reprocessed because they are not compatible with some models.","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2022-09-14T19:56:03.531389Z","iopub.execute_input":"2022-09-14T19:56:03.532051Z","iopub.status.idle":"2022-09-14T19:56:19.555417Z","shell.execute_reply.started":"2022-09-14T19:56:03.532013Z","shell.execute_reply":"2022-09-14T19:56:19.554500Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Missing values statistics\nmissing_values_df = missing_values_table(df)\nmissing_values_df = missing_values_df.sort_values('% of Total Values')\nmissing_values_df[missing_values_df['% of Total Values']>50.0]","metadata":{"execution":{"iopub.status.busy":"2022-09-14T19:56:19.556514Z","iopub.execute_input":"2022-09-14T19:56:19.557419Z","iopub.status.idle":"2022-09-14T19:56:20.609696Z","shell.execute_reply.started":"2022-09-14T19:56:19.557385Z","shell.execute_reply":"2022-09-14T19:56:20.608628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The processing of missing feature values ‚Äã‚Äãwill be done in the modeling part.","metadata":{}}]}